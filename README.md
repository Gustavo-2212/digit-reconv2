# Projeto de Aprendizado de M√°quina com Redes Multicamadas üß†
---
### Autor
- **Nome:** Gustavo Alves de Oliveira
- **Matr√≠cula:** 12311ECP026
- **Disciplina:** Aprendizagem de M√°quina
- **Faculdade:** Universidade Federal de Uberl√¢ndia


ü§ñüìäüîç

Este projeto utiliza t√©cnicas de Aprendizado de M√°quina para treinar uma rede neural para reconhecer d√≠gitos escritos a m√£o. Foi usado a base de dados do [MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset).
Dos 10 arquivos contendo amostras diferentes de 0, 1, 2, ..., 9, foi pego 5 de cada um dos d√≠gitos para treinamento, totalizando em 50 entradas de treinamento. E 1 de cada para teste.\
No diret√≥rio **./data** temos as imagens que foram usadas.

### Redes Neurais Perceptrons de Multicamadas (MLP)

As Redes Neurais Perceptrons de Multicamadas (MLPs) s√£o uma classe de redes neurais artificiais que consistem em uma rede de n√≥s (neur√¥nios) organizados em camadas. As MLPs s√£o compostas por uma camada de entrada, uma ou mais camadas ocultas e uma camada de sa√≠da.

### Funcionamento das MLPs:

1. **Camada de Entrada:**
   - Os n√≥s na camada de entrada representam as caracter√≠sticas (features) dos dados de entrada. Cada n√≥ nesta camada corresponde a uma caracter√≠stica espec√≠fica.

2. **Camadas Ocultas:**
   - As camadas ocultas s√£o compostas por neur√¥nios que processam os dados de entrada e aprendem a representa√ß√£o dos padr√µes nos dados.
   - Cada neur√¥nio em uma camada oculta recebe entradas das camadas anteriores, realiza uma combina√ß√£o linear das entradas ponderadas por pesos, aplica uma fun√ß√£o de ativa√ß√£o n√£o linear e passa o resultado para as camadas posteriores.
   - A presen√ßa de camadas ocultas permite que as MLPs aprendam representa√ß√µes mais complexas e n√£o lineares dos dados.

3. **Camada de Sa√≠da:**
   - A camada de sa√≠da produz a sa√≠da final da rede neural. A estrutura desta camada depende do tipo de problema que est√° sendo abordado, como classifica√ß√£o (por exemplo, softmax para classifica√ß√£o multiclasse) ou regress√£o (um √∫nico neur√¥nio de sa√≠da para prever um valor cont√≠nuo).

>![MLP](https://www.researchgate.net/publication/293013889/figure/fig1/AS:335717596188674@1457052720824/Figura-1-Exemplo-simplificado-de-uma-rede-neural-multicamadas-HAYKIN-2001-Figure-1.png)

### Idealiza√ß√£o e Desenvolvimento:

As MLPs n√£o t√™m um √∫nico idealizador. O desenvolvimento das MLPs foi uma evolu√ß√£o das redes neurais perceptrons originais propostas por Frank Rosenblatt em 1957. As MLPs foram introduzidas para superar a limita√ß√£o das redes perceptrons de uma √∫nica camada, que s√≥ podiam resolver problemas linearmente separ√°veis. A ideia de adicionar camadas ocultas e usar algoritmos de treinamento como o backpropagation permitiu que as MLPs aprendessem a representar rela√ß√µes mais complexas nos dados.

### Fun√ß√£o Sigm√≥ide Bipolar como Fun√ß√£o de Ativa√ß√£o:

A fun√ß√£o sigm√≥ide bipolar, tamb√©m conhecida como fun√ß√£o de ativa√ß√£o bipolar, √© uma fun√ß√£o matem√°tica usada em redes neurais como uma fun√ß√£o de ativa√ß√£o. Sua f√≥rmula √©:

\[ f(x) = \frac{2}{1 + e^{-x}} - 1 \]

Esta fun√ß√£o mapeia os valores de entrada para o intervalo [-1, 1]. Ela √© suave e diferenci√°vel em todos os pontos, o que a torna adequada para o treinamento de redes neurais utilizando algoritmos de otimiza√ß√£o baseados em gradiente, como o backpropagation.

A fun√ß√£o sigm√≥ide bipolar tem sido historicamente utilizada como fun√ß√£o de ativa√ß√£o nos neur√¥nios das camadas ocultas das MLPs. No entanto, devido a problemas como o desaparecimento do gradiente durante o treinamento profundo e a propaga√ß√£o do gradiente muito lenta em camadas profundas, fun√ß√µes de ativa√ß√£o alternativas, como ReLU (Rectified Linear Unit) e suas variantes, tornaram-se mais populares em redes neurais profundas modernas.

> ![Sigm√≥ide Bipolar](https://www.researchgate.net/publication/331087209/figure/fig4/AS:726046831820800@1550114462005/Figura-54-Funcion-de-Activacion-Sigmoide-Bipolar.jpg)

### Estrutura do Projeto

O projeto est√° estruturado da seguinte forma:

- **./data**: Possui as imagens em arquivo visual para treinamento e teste;
- **./src**: Cont√©m os c√≥digos-fonte;
- **./data_bin**: Cont√©m as imagens em formato bin√°rio para a leitura dos dados;
- **./tmp**: Ir√° conter os arquivos de relat√≥rios dos pesos iniciais, pesos finais, valores do erro conforme a rede aprendia e um arquivo **.html** que plota o gr√°fico da m√©dia quadr√°tica.

### Depend√™ncias do Projeto

Para executar o projeto, √© necess√°rio instalar a seguinte biblioteca Python:

- Bokeh: Biblioteca para criar visualiza√ß√µes interativas em navegadores da web.

```bash
pip install bokeh
```

### Como Executar o Projeto

1. Clone o reposit√≥rio do projeto.
2. Instale as depend√™ncias listadas acima.
3. Execute, no diret√≥rio raiz do projeto, os comandos a seguir:

```console
foo@bar:~$ make clean
foo@bar:~$ make exec
```

### Conclus√µes

Podemos ver que a rede neural consegue reconhecer alguns d√≠gitos novos, por√©m poderia se sair bem melhor caso os seguintes recursos fossem desenvolvidos:\
[x] Normaliza√ß√£o dos dados de entrada, uma vez que por ser uma imagem, tem valores de 0 a 255 **[-1.0, 1.0]**;\
[x] Usar mais amostras para treinamento, em vez de apenas 5 de cada d√≠gito **(50 de cada)**;\
[] Testar outras arquiteturas de rede neural;\
Essas s√£o apenas algumas sugest√µes de melhoria.

üöÄüîçüí°

---
